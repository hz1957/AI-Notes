# LLM Inference Optimization

Welcome to the Inference Optimization section. Here you will find guides on how to optimize LLM inference for latency, throughput, and cost.

## Core Concepts
*   [Generation Parameters](./generation-parameters.md): Understanding temperature, top-k, top-p, etc.
*   [KV Cache Strategies](./kv-cache.md): Optimizing memory usage with PagedAttention and more.
*   [Parallelism Strategies](./parallelism.md): Tensor Parallelism vs Pipeline Parallelism.

## Optimization Techniques
*   [Model Optimization](./model-optimization.md): Quantization (AWQ, GPTQ) and pruning.
*   [Speculative Decoding](./speculative-decoding.md): Accelerating generation with draft models.
*   [Serving Techniques](./serving-techniques.md): Batching, continuous batching, and scheduling.

## Advanced Guides
*   [**Advanced RAG Guide**](./advanced_rag_guide.md): Comprehensive engineering patterns for RAG.
*   [SGLang Configuration](./sglang-guide.md): Setting up high-performance serving with SGLang.
